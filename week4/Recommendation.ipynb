{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics of Recommendation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import correlation, cosine\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from numpy import nan as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M_verbose = pd.DataFrame({\n",
    "    \"Desperados\": [4, 1, NaN, 4],\n",
    "    \"Guinness\": [3.0, 2.0, 2.0, 3.0],\n",
    "    \"Chimay Triple\": [2, 3, 1, NaN],\n",
    "    \"Leffe\": [3, 1, NaN, NaN]\n",
    "})\n",
    "\n",
    "M_verbose.index = ['ICT', 'Med', 'Business', 'Enviro']\n",
    "\n",
    "M = np.array([\n",
    "    [4, 3.0, 2, 3], \n",
    "    [1, 2, 3, 1],\n",
    "    [NaN, 2, 1, NaN],\n",
    "    [4, 3, NaN, NaN]\n",
    "])\n",
    "\n",
    "M_df = pd.DataFrame(data=arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 2., 2., 3.])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_similarity(v1,v2, metric='cosine'):\n",
    "    #metric: cosine or correlation\n",
    "    if metric == 'correlation':\n",
    "        v1 = v1 - np.nanmean(v1)\n",
    "        v2 = v2 - np.nanmean(v2)\n",
    "    \"compute similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        if np.isnan(x) or np.isnan(y): continue\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "def sim_matrix(M, dimension='user', metric='cosine'):\n",
    "    N = M.shape[0] if dimension == 'user' else M.shape[1]\n",
    "    sim = np.zeros([N,N])\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                sim[i,j] = 0 #Cancel out the effect of self-similarity in the sums later\n",
    "                continue\n",
    "            if dimension == 'user':\n",
    "                # If user, vector 1 and vector 2 = current iterations (i, j) of users to compare\n",
    "                v1, v2 = M[i,:], M[j,:]\n",
    "            else:\n",
    "                # If item, vector 1 and vector 2 = current iterations (i, j) of drinks to compare\n",
    "                v1, v2 = M[:,i], M[:,j]\n",
    "                \n",
    "            sim[i][j] = cosine_similarity(v1,v2,metric)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9922778767136677"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(M[0,:], M[2,:], 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 3., 2., 3.])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan,  2.,  1., nan])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.79582243, 0.99227788, 1.        ],\n",
       "       [0.79582243, 0.        , 0.86824314, 0.89442719],\n",
       "       [0.99227788, 0.86824314, 0.        , 1.        ],\n",
       "       [1.        , 0.89442719, 1.        , 0.        ]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.9649505 , 0.73994007, 0.99705449],\n",
       "       [0.9649505 , 0.        , 0.90748521, 0.96476382],\n",
       "       [0.73994007, 0.90748521, 0.        , 0.78935222],\n",
       "       [0.99705449, 0.96476382, 0.78935222, 0.        ]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865475"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(M[0,:], M[2,:], 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.85280287,  0.70710678,  0.70710678],\n",
       "       [-0.85280287,  0.        , -0.5547002 , -0.89442719],\n",
       "       [ 0.70710678, -0.5547002 ,  0.        , -1.        ],\n",
       "       [ 0.70710678, -0.89442719, -1.        ,  0.        ]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'user', 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.94280904, -0.89442719,  0.9486833 ],\n",
       "       [ 0.94280904,  0.        ,  0.        ,  1.        ],\n",
       "       [-0.89442719,  0.        ,  0.        , -0.70710678],\n",
       "       [ 0.9486833 ,  1.        , -0.70710678,  0.        ]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'item', 'correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Compute the missing rating in this table using user-based collaborative filtering (CF). (Use cosine similarity, then use Pearson similarity). Assume taking all neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medicine's ratings\n",
      "[1. 2. 3. 1.]\n",
      "Guiness's ratings\n",
      "[3. 2. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Axis 0 = rows, axis 1 = \n",
    "print(\"Medicine's ratings\")\n",
    "print(M[1,:])\n",
    "\n",
    "print(\"Guiness's ratings\")\n",
    "print(M[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def user_cf(M, metric='cosine'):\n",
    "    pred = np.copy(M)\n",
    "    n_users, n_items = M.shape\n",
    "    # Average rating each user gave beers (ignoring nans)\n",
    "    avg_ratings = np.nanmean(M, axis=1)\n",
    "    sim_users = sim_matrix(M, 'user', metric)\n",
    "    \n",
    "    \n",
    "    for i in range(n_users):\n",
    "        for j in range(n_items):\n",
    "            if np.isnan(M[i,j]):\n",
    "\n",
    "                # Can include self sim because 0 will cancel itself\n",
    "                similarities = sim_users[i]\n",
    "                item_ratings = M[:, j]\n",
    "                \n",
    "                numerator = np.nansum(similarities * (item_ratings - avg_ratings))\n",
    "                denominator = np.nansum(similarities)\n",
    "                mean_rating = avg_ratings[i]\n",
    "                \n",
    "#                 print('\\nMean rating: ', mean_rating)\n",
    "#                 print('Numerator: ', numerator)\n",
    "#                 print('Denominator: ', denominator)\n",
    "                \n",
    "                pred[i,j] = mean_rating + (numerator / denominator)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF (Cosine): \n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  1.794036  2.0  1.000000  1.272355\n",
      "3  4.000000  3.0  3.368034  3.268237\n",
      "User-based CF (Pearson): \n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  0.764822  2.0  1.000000  1.009169\n",
      "3  4.000000  3.0  4.616077  2.935013\n"
     ]
    }
   ],
   "source": [
    "print(\"User-based CF (Cosine): \\n\" + str(pd.DataFrame(user_cf(M, 'cosine'))))\n",
    "print(\"User-based CF (Pearson): \\n\" + str(pd.DataFrame(user_cf(M, 'correlation'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Similarly, computing the missing rating using item-based CF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_cf(M, metric='cosine'):\n",
    "    pred = np.copy(M)\n",
    "    n_users, n_items = M.shape\n",
    "    avg_ratings = np.nanmean(M, axis=0)\n",
    "    sim_items = sim_matrix(M, 'item', metric)\n",
    "    for i in range(n_users):\n",
    "        for j in range(n_items):\n",
    "            if np.isnan(M[i,j]):\n",
    "                \n",
    "                similarities = sim_items[j]\n",
    "\n",
    "                users_other_ratings = M[i,:]\n",
    "\n",
    "                mean_rating = avg_ratings[j]\n",
    "                numerator = np.nansum((sim_items[j] * (users_other_ratings - avg_ratings)))\n",
    "                denominator = np.nansum(sim_items[j])\n",
    "                \n",
    "                pred[i,j] = mean_rating + (numerator / denominator)\n",
    "                \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-based CF (Cosine): \n",
      "         0    1         2         3\n",
      "0  4.00000  3.0  2.000000  3.000000\n",
      "1  1.00000  2.0  3.000000  1.000000\n",
      "2  2.54758  2.0  1.000000  1.537748\n",
      "3  4.00000  3.0  2.489861  2.537748\n",
      "Item-based CF (Pearson): \n",
      "          0    1         2        3\n",
      "0  4.000000  3.0  2.000000  3.00000\n",
      "1  1.000000  2.0  3.000000  1.00000\n",
      "2  3.424268  2.0  1.000000  2.16681\n",
      "3  4.000000  3.0  2.558482  3.16681\n"
     ]
    }
   ],
   "source": [
    "print(\"Item-based CF (Cosine): \\n\" + str(pd.DataFrame(item_cf(M, 'cosine'))))\n",
    "print(\"Item-based CF (Pearson): \\n\" + str(pd.DataFrame(item_cf(M, 'correlation'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating Recommendation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  4  3  2  3\n",
       "1  1  2  3  1\n",
       "2  1  2  1  2\n",
       "3  4  3  2  4"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_result = np.asarray([[4,3,2,3], \n",
    "                [1,2,3,1],\n",
    "                [1,2,1,2],\n",
    "                [4,3,2,4]])\n",
    "pd.DataFrame(M_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def evaluateRS(ratings, groundtruth, method='user_cf', metric='cosine'):\n",
    "    #method: user_cf and item_cf, metric: cosine and correlation\n",
    "    if method == 'user_cf':\n",
    "        prediction = user_cf(ratings, metric)\n",
    "    else:\n",
    "        prediction = item_cf(ratings, metric)\n",
    "    MSE = mean_squared_error(prediction, groundtruth)\n",
    "    RMSE = round(sqrt(MSE),3)\n",
    "    print(\"RMSE using {0} approach ({2}) is: {1}\".format(method, RMSE, metric))\n",
    "    print(pd.DataFrame(prediction))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE using user_cf approach (cosine) is: 0.472\n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  1.794036  2.0  1.000000  1.272355\n",
      "3  4.000000  3.0  3.368034  3.268237\n",
      "RMSE using user_cf approach (correlation) is: 0.751\n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  0.764822  2.0  1.000000  1.009169\n",
      "3  4.000000  3.0  4.616077  2.935013\n",
      "RMSE using item_cf approach (cosine) is: 0.558\n",
      "         0    1         2         3\n",
      "0  4.00000  3.0  2.000000  3.000000\n",
      "1  1.00000  2.0  3.000000  1.000000\n",
      "2  2.54758  2.0  1.000000  1.537748\n",
      "3  4.00000  3.0  2.489861  2.537748\n",
      "RMSE using item_cf approach (correlation) is: 0.657\n",
      "          0    1         2        3\n",
      "0  4.000000  3.0  2.000000  3.00000\n",
      "1  1.000000  2.0  3.000000  1.00000\n",
      "2  3.424268  2.0  1.000000  2.16681\n",
      "3  4.000000  3.0  2.558482  3.16681\n"
     ]
    }
   ],
   "source": [
    "evaluateRS(M, M_result)\n",
    "evaluateRS(M, M_result, metric='correlation')\n",
    "evaluateRS(M, M_result, method='item_cf')\n",
    "evaluateRS(M, M_result, method='item_cf', metric='correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank accuracy user with cosine metric:  0.6477056190747297\n",
      "Rank accuracy user with correlation metric: 0.56719350585564\n",
      "Rank accuracy item with cosine metric:  0.6369306393762916\n",
      "Rank accuracy item with correlation metric:  0.7282177322938193\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def evaluate_rank(ratings, groundtruth, n_users, method='user_cf', metric='cosine'):\n",
    "    #metric: cosine vs correlation\n",
    "    if method == 'user_cf':\n",
    "        prediction = user_cf(ratings, metric)\n",
    "    else:\n",
    "        prediction = item_cf(ratings, metric)\n",
    "    \n",
    "    avg_tau = 0\n",
    "\n",
    "    for i in range(n_users):\n",
    "        tau, p_value = stats.kendalltau(M_result[i,:], prediction[i,:])\n",
    "        avg_tau += tau\n",
    "    avg_tau = avg_tau / n_users\n",
    "    clear_output(wait=True)\n",
    "    return avg_tau\n",
    "\n",
    "cosine_user = evaluate_rank(M, M_result, 4)\n",
    "correlation_user = evaluate_rank(M, M_result, 4, metric='correlation')\n",
    "cosine_item = evaluate_rank(M, M_result, 4, method='item')\n",
    "correlation_item = evaluate_rank(M, M_result, 4, method='item', metric='correlation')\n",
    "\n",
    "print(\"Rank accuracy user with cosine metric: \", cosine_user)\n",
    "print(\"Rank accuracy user with correlation metric:\", correlation_user)\n",
    "print(\"Rank accuracy item with cosine metric: \", cosine_item)\n",
    "print(\"Rank accuracy item with correlation metric: \", correlation_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
